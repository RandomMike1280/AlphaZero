# AlphaZero Implementation

This repository contains a PyTorch implementation of DeepMind's AlphaZero algorithm for both Tic Tac Toe and Chess. AlphaZero learns to play games through self-play, using Monte Carlo Tree Search (MCTS) guided by a deep neural network.

## Features

- Clean, modular implementation of AlphaZero
- Support for both Tic Tac Toe and Chess games
- Residual neural network architecture
- MCTS with PUCT exploration formula
- Self-play with Dirichlet noise for exploration
- Training loop with TensorBoard integration
- Command-line interface for training and playing

## Project Structure

```
├── alphazero/
│   ├── games/
│   │   ├── game.py           # Abstract game class
│   │   ├── tictactoe.py      # Tic Tac Toe implementation
│   │   └── chess_game.py     # Chess implementation
│   ├── mcts/
│   │   ├── node.py           # MCTS node implementation
│   │   └── search.py         # MCTS search algorithm
│   ├── neural_network/
│   │   ├── model.py          # Neural network architecture
│   │   └── training.py       # Training utilities
│   ├── self_play.py          # Self-play data generation
│   ├── trainer.py            # Main training loop
│   └── utils.py              # Helper functions
├── models/                   # Save/load models directory
├── logs/                     # Training logs directory
├── train_tictactoe.py        # Quick training on Tic Tac Toe
├── train_chess.py            # Full training on Chess
├── play_tictactoe.py         # Play against Tic Tac Toe model
├── play_chess.py             # Play against Chess model
└── requirements.txt          # Project dependencies
```

## Installation

1. Clone this repository:
```bash
git clone https://github.com/yourusername/alphazero.git
cd alphazero
```

2. Create a virtual environment and install dependencies:
```bash
python -m venv venv
source venv/bin/activate  # On Windows, use: venv\Scripts\activate
pip install -r requirements.txt
```

## Training Models

### Tic Tac Toe

Train a Tic Tac Toe model with default parameters:

```bash
python train_tictactoe.py
```

Customize training parameters:

```bash
python train_tictactoe.py --num_iterations 10 --num_selfplay_iterations 50 --num_epochs 20 --batch_size 128
```

### Chess

Train a Chess model with default parameters:

```bash
python train_chess.py
```

Customize training parameters:

```bash
python train_chess.py --num_iterations 100 --num_selfplay_iterations 500 --num_resblocks 19 --num_filters 256
```

## Playing Against Trained Models

### Tic Tac Toe

Play against a trained Tic Tac Toe model:

```bash
python play_tictactoe.py --model_path models/tictactoe_final_model.pt
```

### Chess

Play against a trained Chess model:

```bash
python play_chess.py --model_path models/chess/chess_final_model.pt --human_player white
```

## Training Parameters

The training process can be customized with several parameters:

- `--num_iterations`: Number of training iterations
- `--num_selfplay_iterations`: Number of self-play games per iteration
- `--num_epochs`: Number of training epochs per iteration
- `--batch_size`: Batch size for training
- `--lr`: Learning rate
- `--num_resblocks`: Number of residual blocks in neural network
- `--num_filters`: Number of filters in convolutional layers
- `--num_simulations`: Number of MCTS simulations per move
- `--c_puct`: Exploration constant in PUCT formula
- `--dirichlet_alpha`: Alpha parameter for Dirichlet noise
- `--temperature_threshold`: Move number to decrease temperature

## Implementation Details

This implementation follows the AlphaZero algorithm described in the DeepMind papers:

1. The neural network consists of a convolutional layer followed by residual blocks, with two heads: a policy head that predicts move probabilities and a value head that evaluates positions.

2. MCTS uses the PUCT formula for balancing exploration and exploitation:
   - Q(s,a) + c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))

3. During self-play, Dirichlet noise is added to the prior probabilities at the root node to encourage exploration.

4. The neural network is trained on data generated by self-play, using a combined loss function:
   - L = (z - v)² - π^T log(p) + c||θ||²

## License

This project is licensed under the MIT License.

## Acknowledgments

This implementation was inspired by DeepMind's AlphaZero papers and various open-source implementations in the community.

## References

- Silver, D., Schrittwieser, J., Simonyan, K. et al. "Mastering the game of Go without human knowledge". Nature 550, 354–359 (2017).
- Silver, D., Hubert, T., Schrittwieser, J. et al. "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play". Science 362, 1140-1144 (2018). 